{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model-to-model comparison\n",
    "\n",
    "You can also use Brain-Score to compare how similar models are to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavioral comparison\n",
    "\n",
    "Let's compare the reading times predictions of two models:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 1 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 16>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# compare\u001B[39;00m\n\u001B[1;32m     15\u001B[0m metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpearsonr\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m score \u001B[38;5;241m=\u001B[39m \u001B[43mmetric\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreading_times1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreading_times2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(score)\n",
      "File \u001B[0;32m~/brain-score_language/brainscore_language/metrics/pearson_correlation/__init__.py:15\u001B[0m, in \u001B[0;36mPearsonCorrelation.__call__\u001B[0;34m(self, assembly1, assembly2)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, assembly1: DataAssembly, assembly2: DataAssembly) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Score:\n\u001B[0;32m---> 15\u001B[0m     rvalue, pvalue \u001B[38;5;241m=\u001B[39m \u001B[43mpearsonr\u001B[49m\u001B[43m(\u001B[49m\u001B[43massembly1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43massembly2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     score \u001B[38;5;241m=\u001B[39m Score(np\u001B[38;5;241m.\u001B[39mabs(rvalue))\n\u001B[1;32m     17\u001B[0m     score\u001B[38;5;241m.\u001B[39mattrs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrvalue\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m rvalue\n",
      "File \u001B[0;32m~/miniconda3/envs/brainscore_language/lib/python3.8/site-packages/scipy/stats/stats.py:3912\u001B[0m, in \u001B[0;36mpearsonr\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m   3909\u001B[0m y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(y)\n\u001B[1;32m   3911\u001B[0m \u001B[38;5;66;03m# If an input is constant, the correlation coefficient is not defined.\u001B[39;00m\n\u001B[0;32m-> 3912\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (x \u001B[38;5;241m==\u001B[39m \u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m)\u001B[38;5;241m.\u001B[39mall() \u001B[38;5;129;01mor\u001B[39;00m (y \u001B[38;5;241m==\u001B[39m y[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m.\u001B[39mall():\n\u001B[1;32m   3913\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(PearsonRConstantInputWarning())\n\u001B[1;32m   3914\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan, np\u001B[38;5;241m.\u001B[39mnan\n",
      "\u001B[0;31mIndexError\u001B[0m: too many indices for array: array is 0-dimensional, but 1 were indexed"
     ]
    }
   ],
   "source": [
    "from brainscore_language import load_model, ArtificialSubject, load_metric\n",
    "\n",
    "# load models\n",
    "model1 = load_model('distilgpt2')\n",
    "model2 = load_model('gpt2-xl')\n",
    "\n",
    "# perform task\n",
    "model1.perform_behavioral_task(ArtificialSubject.Task.reading_times)\n",
    "model2.perform_behavioral_task(ArtificialSubject.Task.reading_times)\n",
    "text = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "reading_times1 = model1.digest_text(text)\n",
    "reading_times2 = model2.digest_text(text)\n",
    "\n",
    "# compare\n",
    "metric = load_metric('pearsonr')\n",
    "score = metric(reading_times1, reading_times2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural comparison\n",
    "\n",
    "Similarly, you can compare how similar neural activity is in two models.\n",
    "Here, we compare two artificial subject models stemming from the same base model by choosing different layers, but you can also compare different models altogether like above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from brainscore_language import ArtificialSubject, load_metric\n",
    "from brainscore_language.model_helpers.huggingface import HuggingfaceSubject\n",
    "\n",
    "# load models\n",
    "model1 = HuggingfaceSubject(model_id='distilgpt2', region_layer_mapping={\n",
    "        ArtificialSubject.RecordingTarget.language_system: 'transformer.h.4.ln_1'})\n",
    "model2 = HuggingfaceSubject(model_id='distilgpt2', region_layer_mapping={\n",
    "        ArtificialSubject.RecordingTarget.language_system: 'transformer.h.5.ln_1'})\n",
    "\n",
    "# record neural activity\n",
    "model1.perform_neural_recording(recording_target=ArtificialSubject.RecordingTarget.language_system,\n",
    "                                recording_type=ArtificialSubject.RecordingType.fMRI)\n",
    "model2.perform_neural_recording(recording_target=ArtificialSubject.RecordingTarget.language_system,\n",
    "                                recording_type=ArtificialSubject.RecordingType.fMRI)\n",
    "text = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "activity1 = model1.digest_text(text)\n",
    "activity2 = model2.digest_text(text)\n",
    "\n",
    "# compare\n",
    "metric = load_metric('linear_pearsonr')\n",
    "score = metric(activity1, activity2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Human-to-human comparison\n",
    "\n",
    "As with the model-to-model comparisons, you can compare humans to one another.\n",
    "In this case, the data has already been recorded so we simply compare two sets of data to one another.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavioral comparison\n",
    "\n",
    "Using data from Futrell et al. 2018, we can compare how similar the reading times of half the subjects are to the reading times of the other half of subjects (this is also part of how the ceiling is estimated in the [Futrell2018 reading times benchmark](https://github.com/brain-score/language/blob/main/brainscore_language/benchmarks/futrell2018/__init__.py)):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "from brainscore_language import load_dataset, load_metric\n",
    "\n",
    "# load data\n",
    "data = load_dataset('Futrell2018')\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split into halves\n",
    "random = RandomState(0)\n",
    "subjects = data['subject_id'].values\n",
    "half1_subjects = random.choice(subjects, size=len(subjects) // 2, replace=False)\n",
    "half2_subjects = set(subjects) - set(half1_subjects)\n",
    "half1 = data.sel(subject_id=half1_subjects)\n",
    "half2 = data.sel(subject_id=half2_subjects) # todo: or data[{'subject': [subject_id in half1_subjects for subject_id in subjects]}]\n",
    "\n",
    "# compare\n",
    "metric = load_metric('pearsonr')\n",
    "score = metric(half1, half2)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
