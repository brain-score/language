{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model-to-model comparison\n",
    "\n",
    "You can also use Brain-Score to compare how similar models are to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavioral comparison\n",
    "\n",
    "Let's compare the reading times predictions of two models:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score ()>\n",
      "array(0.75950604)\n",
      "Attributes:\n",
      "    rvalue:   0.7595060423938943\n",
      "    pvalue:   0.028803341212904062\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from brainscore_language import load_model, ArtificialSubject, load_metric\n",
    "\n",
    "# load models\n",
    "model1 = load_model('distilgpt2')\n",
    "model2 = load_model('gpt2-xl')\n",
    "\n",
    "# perform task\n",
    "model1.perform_behavioral_task(ArtificialSubject.Task.reading_times)\n",
    "model2.perform_behavioral_task(ArtificialSubject.Task.reading_times)\n",
    "text = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "reading_times1 = model1.digest_text(text)['behavior'][1:]\n",
    "reading_times2 = model2.digest_text(text)['behavior'][1:]\n",
    "\n",
    "# compare\n",
    "metric = load_metric('pearsonr')\n",
    "score = metric(reading_times1, reading_times2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural comparison\n",
    "\n",
    "Similarly, you can compare how similar neural activity is in two models.\n",
    "Here, we compare two artificial subject models stemming from the same base model by choosing different layers, but you can also compare different models altogether like above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "cross-validation:   0%|                                                    | 0/10 [00:00<?, ?it/s]\u001B[A\n",
      "cross-validation:  10%|████▍                                       | 1/10 [00:01<00:09,  1.11s/it]\u001B[A\n",
      "cross-validation:  20%|████████▊                                   | 2/10 [00:02<00:09,  1.18s/it]\u001B[A\n",
      "cross-validation:  30%|█████████████▏                              | 3/10 [00:03<00:08,  1.28s/it]\u001B[A\n",
      "cross-validation:  40%|█████████████████▌                          | 4/10 [00:04<00:07,  1.25s/it]\u001B[A\n",
      "cross-validation:  50%|██████████████████████                      | 5/10 [00:06<00:06,  1.24s/it]\u001B[A\n",
      "cross-validation:  60%|██████████████████████████▍                 | 6/10 [00:07<00:04,  1.24s/it]\u001B[A\n",
      "cross-validation:  70%|██████████████████████████████▊             | 7/10 [00:08<00:03,  1.24s/it]\u001B[A\n",
      "cross-validation:  80%|███████████████████████████████████▏        | 8/10 [00:09<00:02,  1.16s/it]\u001B[A\n",
      "cross-validation:  90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.16s/it]\u001B[A\n",
      "cross-validation: 100%|███████████████████████████████████████████| 10/10 [00:11<00:00,  1.19s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score ()>\n",
      "array(0.62209412)\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (split: 10, neuroid: 768)>\\narray([[ 0.95910202, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from brainscore_language import ArtificialSubject, load_metric\n",
    "from brainscore_language.model_helpers.huggingface import HuggingfaceSubject\n",
    "\n",
    "# load models\n",
    "model1 = HuggingfaceSubject(model_id='distilgpt2', region_layer_mapping={\n",
    "    ArtificialSubject.RecordingTarget.language_system: 'transformer.h.4.ln_1'})\n",
    "model2 = HuggingfaceSubject(model_id='distilgpt2', region_layer_mapping={\n",
    "    ArtificialSubject.RecordingTarget.language_system: 'transformer.h.5.ln_1'})\n",
    "\n",
    "# record neural activity\n",
    "model1.perform_neural_recording(recording_target=ArtificialSubject.RecordingTarget.language_system,\n",
    "                                recording_type=ArtificialSubject.RecordingType.fMRI)\n",
    "model2.perform_neural_recording(recording_target=ArtificialSubject.RecordingTarget.language_system,\n",
    "                                recording_type=ArtificialSubject.RecordingType.fMRI)\n",
    "text = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog',\n",
    "        'Waltz', 'bad', 'nymph', 'for', 'quick', 'jigs', 'vex',\n",
    "        \"Glib\", \"jocks\", \"quiz\", \"nymph\", \"to\", \"vex\", \"dwarf\",\n",
    "        \"Sphinx\", \"of\", \"black\", \"quartz,\", \"judge\", \"my\", \"vow\",\n",
    "        \"How\", \"vexingly\", \"quick\", \"daft\", \"zebras\", \"jump!\"]\n",
    "activity1 = model1.digest_text(text)['neural']\n",
    "activity2 = model2.digest_text(text)['neural']\n",
    "activity1['stimulus_id'] = activity2['stimulus_id'] = 'presentation', np.arange(len(text))\n",
    "\n",
    "# compare\n",
    "metric = load_metric('linear_pearsonr')\n",
    "score = metric(activity1, activity2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Human-to-human comparison\n",
    "\n",
    "As with the model-to-model comparisons, you can compare humans to one another.\n",
    "In this case, the data has already been recorded so we simply compare two sets of data to one another.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavioral comparison\n",
    "\n",
    "Using data from Futrell et al. 2018, we can compare how similar the reading times of half the subjects are to the reading times of the other half of subjects (this is also part of how the ceiling is estimated in the [Futrell2018 reading times benchmark](https://github.com/brain-score/language/blob/main/brainscore_language/benchmarks/futrell2018/__init__.py)):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.NeuroidAssembly 'data' (presentation: 10256, subject: 180)>\n",
      "array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       ...,\n",
      "       [512., 334., 283., ...,  nan,  nan,  nan],\n",
      "       [432., 390., 590., ...,  nan,  nan,  nan],\n",
      "       [576., 750., 862., ...,  nan,  nan,  nan]])\n",
      "Coordinates:\n",
      "  * presentation             (presentation) MultiIndex\n",
      "  - word                     (presentation) object 'If' 'you' ... \"Tourette's.\"\n",
      "  - word_core                (presentation) object 'If' 'you' ... 'Tourettes'\n",
      "  - story_id                 (presentation) int64 1 1 1 1 1 1 ... 10 10 10 10 10\n",
      "  - word_id                  (presentation) int64 1 2 3 4 5 ... 936 937 938 939\n",
      "  - word_within_sentence_id  (presentation) int64 1 2 3 4 5 6 ... 12 13 14 15 16\n",
      "  - sentence_id              (presentation) int64 1 1 1 1 1 ... 481 481 481 481\n",
      "  - stimulus_id              (presentation) int64 1 2 3 4 ... 10254 10255 10256\n",
      "  * subject                  (subject) MultiIndex\n",
      "  - subject_id               (subject) object 'A1INWCFGQI236V' ... 'A2VG5S4UL...\n",
      "  - correct                  (subject) int64 6 6 5 6 6 6 6 5 ... 6 5 5 6 6 6 6 5\n",
      "  - WorkTimeInSeconds        (subject) int64 1569 2748 2409 ... 1174 523 370\n",
      "Attributes:\n",
      "    identifier:  Futrell2018\n",
      "    bibtex:      @proceedings{futrell2018natural,\\n  title={The Natural Stori...\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import RandomState\n",
    "from brainscore_language import load_dataset, load_metric\n",
    "\n",
    "# load data\n",
    "data = load_dataset('Futrell2018')\n",
    "print(data)  # will show lots of nans because not every subject has a reading time for every word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score ()>\n",
      "array(0.61563052)\n",
      "Attributes:\n",
      "    rvalue:   0.6156305242624502\n",
      "    pvalue:   0.0\n"
     ]
    }
   ],
   "source": [
    "# split into halves\n",
    "random = RandomState(0)\n",
    "subjects = data['subject_id'].values\n",
    "half1_subjects = random.choice(subjects, size=len(subjects) // 2, replace=False)\n",
    "half2_subjects = set(subjects) - set(half1_subjects)\n",
    "half1 = data[{'subject': [subject_id in half1_subjects for subject_id in subjects]}]\n",
    "half2 = data[{'subject': [subject_id in half2_subjects for subject_id in subjects]}]\n",
    "\n",
    "# mean within each half\n",
    "half1 = half1.mean('subject')\n",
    "half2 = half2.mean('subject')\n",
    "\n",
    "# compare\n",
    "metric = load_metric('pearsonr')\n",
    "score = metric(half1, half2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
