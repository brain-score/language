{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model-to-model comparison\n",
    "\n",
    "You can also use Brain-Score to compare how similar models are to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavioral comparison\n",
    "\n",
    "Let's compare the reading times predictions of two models:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 16>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# compare\u001B[39;00m\n\u001B[1;32m     15\u001B[0m metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpearsonr\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m score \u001B[38;5;241m=\u001B[39m \u001B[43mmetric\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreading_times1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreading_times2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(score)\n",
      "File \u001B[0;32m~/brain-score_language/brainscore_language/metrics/pearson_correlation/__init__.py:15\u001B[0m, in \u001B[0;36mPearsonCorrelation.__call__\u001B[0;34m(self, assembly1, assembly2)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, assembly1: DataAssembly, assembly2: DataAssembly) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Score:\n\u001B[0;32m---> 15\u001B[0m     rvalue, pvalue \u001B[38;5;241m=\u001B[39m \u001B[43mpearsonr\u001B[49m\u001B[43m(\u001B[49m\u001B[43massembly1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43massembly2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     score \u001B[38;5;241m=\u001B[39m Score(np\u001B[38;5;241m.\u001B[39mabs(rvalue))\n\u001B[1;32m     17\u001B[0m     score\u001B[38;5;241m.\u001B[39mattrs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrvalue\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m rvalue\n",
      "File \u001B[0;32m~/miniconda3/envs/brainscore_language/lib/python3.8/site-packages/scipy/stats/stats.py:3935\u001B[0m, in \u001B[0;36mpearsonr\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m   3930\u001B[0m ym \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mastype(dtype) \u001B[38;5;241m-\u001B[39m ymean\n\u001B[1;32m   3932\u001B[0m \u001B[38;5;66;03m# Unlike np.linalg.norm or the expression sqrt((xm*xm).sum()),\u001B[39;00m\n\u001B[1;32m   3933\u001B[0m \u001B[38;5;66;03m# scipy.linalg.norm(xm) does not overflow if xm is, for example,\u001B[39;00m\n\u001B[1;32m   3934\u001B[0m \u001B[38;5;66;03m# [-5e210, 5e210, 3e200, -3e200]\u001B[39;00m\n\u001B[0;32m-> 3935\u001B[0m normxm \u001B[38;5;241m=\u001B[39m \u001B[43mlinalg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3936\u001B[0m normym \u001B[38;5;241m=\u001B[39m linalg\u001B[38;5;241m.\u001B[39mnorm(ym)\n\u001B[1;32m   3938\u001B[0m threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-13\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/brainscore_language/lib/python3.8/site-packages/scipy/linalg/misc.py:140\u001B[0m, in \u001B[0;36mnorm\u001B[0;34m(a, ord, axis, keepdims, check_finite)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;66;03m# Differs from numpy only in non-finite handling and the use of blas.\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_finite:\n\u001B[0;32m--> 140\u001B[0m     a \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray_chkfinite\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    142\u001B[0m     a \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(a)\n",
      "File \u001B[0;32m~/miniconda3/envs/brainscore_language/lib/python3.8/site-packages/numpy/lib/function_base.py:488\u001B[0m, in \u001B[0;36masarray_chkfinite\u001B[0;34m(a, dtype, order)\u001B[0m\n\u001B[1;32m    486\u001B[0m a \u001B[38;5;241m=\u001B[39m asarray(a, dtype\u001B[38;5;241m=\u001B[39mdtype, order\u001B[38;5;241m=\u001B[39morder)\n\u001B[1;32m    487\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m a\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mchar \u001B[38;5;129;01min\u001B[39;00m typecodes[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAllFloat\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misfinite(a)\u001B[38;5;241m.\u001B[39mall():\n\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray must not contain infs or NaNs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    490\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m a\n",
      "\u001B[0;31mValueError\u001B[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "from brainscore_language import load_model, ArtificialSubject, load_metric\n",
    "\n",
    "# load models\n",
    "model1 = load_model('distilgpt2')\n",
    "model2 = load_model('gpt2-xl')\n",
    "\n",
    "# perform task\n",
    "model1.perform_behavioral_task(ArtificialSubject.Task.reading_times)\n",
    "model2.perform_behavioral_task(ArtificialSubject.Task.reading_times)\n",
    "text = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "reading_times1 = model1.digest_text(text)['behavior']\n",
    "reading_times2 = model2.digest_text(text)['behavior']\n",
    "\n",
    "# compare\n",
    "metric = load_metric('pearsonr')\n",
    "score = metric(reading_times1, reading_times2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural comparison\n",
    "\n",
    "Similarly, you can compare how similar neural activity is in two models.\n",
    "Here, we compare two artificial subject models stemming from the same base model by choosing different layers, but you can also compare different models altogether like above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'stimulus_id'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/brainscore_language/lib/python3.8/site-packages/xarray/core/dataarray.py:693\u001B[0m, in \u001B[0;36mDataArray._getitem_coord\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    692\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 693\u001B[0m     var \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_coords\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "\u001B[0;31mKeyError\u001B[0m: 'stimulus_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 21>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# compare\u001B[39;00m\n\u001B[1;32m     20\u001B[0m metric \u001B[38;5;241m=\u001B[39m load_metric(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinear_pearsonr\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m score \u001B[38;5;241m=\u001B[39m \u001B[43mmetric\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactivity1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivity2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(score)\n",
      "File \u001B[0;32m~/brain-score_language/brainscore_language/metrics/linear_predictivity/__init__.py:118\u001B[0m, in \u001B[0;36mCrossRegressedCorrelation.__call__\u001B[0;34m(self, assembly1, assembly2)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, assembly1: DataAssembly, assembly2: DataAssembly) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Score:\n\u001B[0;32m--> 118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43massembly1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43massembly2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapply\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maggregate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maggregate\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/brain-score_language/brainscore_language/utils/transformations.py:22\u001B[0m, in \u001B[0;36mTransformation.__call__\u001B[0;34m(self, apply, aggregate, *args, **kwargs)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, apply, aggregate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Score:\n\u001B[0;32m---> 22\u001B[0m     values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_pipe\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapply\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     score \u001B[38;5;241m=\u001B[39m apply_aggregate(aggregate, values) \u001B[38;5;28;01mif\u001B[39;00m aggregate \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m values\n\u001B[1;32m     25\u001B[0m     score \u001B[38;5;241m=\u001B[39m apply_aggregate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maggregate, score)\n",
      "File \u001B[0;32m~/brain-score_language/brainscore_language/utils/transformations.py:30\u001B[0m, in \u001B[0;36mTransformation._run_pipe\u001B[0;34m(self, apply, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_pipe\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, apply, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     29\u001B[0m     generator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpipe(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m vals \u001B[38;5;129;01min\u001B[39;00m generator:\n\u001B[1;32m     31\u001B[0m         y \u001B[38;5;241m=\u001B[39m apply(\u001B[38;5;241m*\u001B[39mvals)\n\u001B[1;32m     32\u001B[0m         done \u001B[38;5;241m=\u001B[39m generator\u001B[38;5;241m.\u001B[39msend(y)\n",
      "File \u001B[0;32m~/brain-score_language/brainscore_language/utils/transformations.py:194\u001B[0m, in \u001B[0;36mCrossValidation.pipe\u001B[0;34m(self, source_assembly, target_assembly)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpipe\u001B[39m(\u001B[38;5;28mself\u001B[39m, source_assembly, target_assembly):\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# check only for equal values, alignment is given by metadata\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(\u001B[43msource_assembly\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_split_coord\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mvalues) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28msorted\u001B[39m(target_assembly[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_split_coord]\u001B[38;5;241m.\u001B[39mvalues)\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_split\u001B[38;5;241m.\u001B[39mdo_stratify:\n\u001B[1;32m    196\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(source_assembly, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stratification_coord)\n",
      "File \u001B[0;32m~/miniconda3/envs/brainscore_language/lib/python3.8/site-packages/xarray/core/dataarray.py:704\u001B[0m, in \u001B[0;36mDataArray.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataArray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 704\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_coord\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;66;03m# xarray-style array indexing\u001B[39;00m\n\u001B[1;32m    707\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39misel(indexers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_item_key_to_dict(key))\n",
      "File \u001B[0;32m~/miniconda3/envs/brainscore_language/lib/python3.8/site-packages/xarray/core/dataarray.py:696\u001B[0m, in \u001B[0;36mDataArray._getitem_coord\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[1;32m    695\u001B[0m     dim_sizes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdims, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape))\n\u001B[0;32m--> 696\u001B[0m     _, key, var \u001B[38;5;241m=\u001B[39m \u001B[43m_get_virtual_variable\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_coords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_level_coords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim_sizes\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    700\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_replace_maybe_drop_dims(var, name\u001B[38;5;241m=\u001B[39mkey)\n",
      "File \u001B[0;32m~/miniconda3/envs/brainscore_language/lib/python3.8/site-packages/xarray/core/dataset.py:171\u001B[0m, in \u001B[0;36m_get_virtual_variable\u001B[0;34m(variables, key, level_vars, dim_sizes)\u001B[0m\n\u001B[1;32m    169\u001B[0m     ref_var \u001B[38;5;241m=\u001B[39m dim_var\u001B[38;5;241m.\u001B[39mto_index_variable()\u001B[38;5;241m.\u001B[39mget_level_variable(ref_name)\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 171\u001B[0m     ref_var \u001B[38;5;241m=\u001B[39m \u001B[43mvariables\u001B[49m\u001B[43m[\u001B[49m\u001B[43mref_name\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m var_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    174\u001B[0m     virtual_var \u001B[38;5;241m=\u001B[39m ref_var\n",
      "\u001B[0;31mKeyError\u001B[0m: 'stimulus_id'"
     ]
    }
   ],
   "source": [
    "from brainscore_language import ArtificialSubject, load_metric\n",
    "from brainscore_language.model_helpers.huggingface import HuggingfaceSubject\n",
    "\n",
    "# load models\n",
    "model1 = HuggingfaceSubject(model_id='distilgpt2', region_layer_mapping={\n",
    "        ArtificialSubject.RecordingTarget.language_system: 'transformer.h.4.ln_1'})\n",
    "model2 = HuggingfaceSubject(model_id='distilgpt2', region_layer_mapping={\n",
    "        ArtificialSubject.RecordingTarget.language_system: 'transformer.h.5.ln_1'})\n",
    "\n",
    "# record neural activity\n",
    "model1.perform_neural_recording(recording_target=ArtificialSubject.RecordingTarget.language_system,\n",
    "                                recording_type=ArtificialSubject.RecordingType.fMRI)\n",
    "model2.perform_neural_recording(recording_target=ArtificialSubject.RecordingTarget.language_system,\n",
    "                                recording_type=ArtificialSubject.RecordingType.fMRI)\n",
    "text = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "activity1 = model1.digest_text(text)['neural']\n",
    "activity2 = model2.digest_text(text)['neural']\n",
    "\n",
    "# compare\n",
    "metric = load_metric('linear_pearsonr')\n",
    "score = metric(activity1, activity2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Human-to-human comparison\n",
    "\n",
    "As with the model-to-model comparisons, you can compare humans to one another.\n",
    "In this case, the data has already been recorded so we simply compare two sets of data to one another.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavioral comparison\n",
    "\n",
    "Using data from Futrell et al. 2018, we can compare how similar the reading times of half the subjects are to the reading times of the other half of subjects (this is also part of how the ceiling is estimated in the [Futrell2018 reading times benchmark](https://github.com/brain-score/language/blob/main/brainscore_language/benchmarks/futrell2018/__init__.py)):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.NeuroidAssembly 'data' (presentation: 10256, subject: 180)>\n",
      "array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       ...,\n",
      "       [512., 334., 283., ...,  nan,  nan,  nan],\n",
      "       [432., 390., 590., ...,  nan,  nan,  nan],\n",
      "       [576., 750., 862., ...,  nan,  nan,  nan]])\n",
      "Coordinates:\n",
      "  * presentation             (presentation) MultiIndex\n",
      "  - word                     (presentation) object 'If' 'you' ... \"Tourette's.\"\n",
      "  - word_core                (presentation) object 'If' 'you' ... 'Tourettes'\n",
      "  - story_id                 (presentation) int64 1 1 1 1 1 1 ... 10 10 10 10 10\n",
      "  - word_id                  (presentation) int64 1 2 3 4 5 ... 936 937 938 939\n",
      "  - word_within_sentence_id  (presentation) int64 1 2 3 4 5 6 ... 12 13 14 15 16\n",
      "  - sentence_id              (presentation) int64 1 1 1 1 1 ... 481 481 481 481\n",
      "  - stimulus_id              (presentation) int64 1 2 3 4 ... 10254 10255 10256\n",
      "  * subject                  (subject) MultiIndex\n",
      "  - subject_id               (subject) object 'A1INWCFGQI236V' ... 'A2VG5S4UL...\n",
      "  - correct                  (subject) int64 6 6 5 6 6 6 6 5 ... 6 5 5 6 6 6 6 5\n",
      "  - WorkTimeInSeconds        (subject) int64 1569 2748 2409 ... 1174 523 370\n",
      "Attributes:\n",
      "    identifier:  Futrell2018\n",
      "    bibtex:      @proceedings{futrell2018natural,\\n  title={The Natural Stori...\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import RandomState\n",
    "from brainscore_language import load_dataset, load_metric\n",
    "\n",
    "# load data\n",
    "data = load_dataset('Futrell2018')\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score ()>\n",
      "array(0.61563052)\n",
      "Attributes:\n",
      "    rvalue:   0.6156305242624502\n",
      "    pvalue:   0.0\n"
     ]
    }
   ],
   "source": [
    "# split into halves\n",
    "random = RandomState(0)\n",
    "subjects = data['subject_id'].values\n",
    "half1_subjects = random.choice(subjects, size=len(subjects) // 2, replace=False)\n",
    "half2_subjects = set(subjects) - set(half1_subjects)\n",
    "half1 = data[{'subject': [subject_id in half1_subjects for subject_id in subjects]}]\n",
    "half2 = data[{'subject': [subject_id in half2_subjects for subject_id in subjects]}]\n",
    "\n",
    "# mean within each half\n",
    "half1 = half1.mean('subject')\n",
    "half2 = half2.mean('subject')\n",
    "\n",
    "# compare\n",
    "metric = load_metric('pearsonr')\n",
    "score = metric(half1, half2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
