{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model-to-model comparison\n",
    "\n",
    "You can also use Brain-Score to compare how similar models are to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavioral comparison\n",
    "\n",
    "Let's compare the reading times predictions of two models:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score ()>\n",
      "array(0.75950604)\n",
      "Attributes:\n",
      "    rvalue:   0.7595060423938943\n",
      "    pvalue:   0.028803341212904062\n"
     ]
    }
   ],
   "source": [
    "from brainscore_language import load_model, ArtificialSubject, load_metric\n",
    "\n",
    "# load models\n",
    "model1 = load_model('distilgpt2')\n",
    "model2 = load_model('gpt2-xl')\n",
    "\n",
    "# start task\n",
    "model1.start_behavioral_task(ArtificialSubject.Task.reading_times)\n",
    "model2.start_behavioral_task(ArtificialSubject.Task.reading_times)\n",
    "text = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
    "reading_times1 = model1.digest_text(text)['behavior'][1:]  # use all reading times except for first word\n",
    "reading_times2 = model2.digest_text(text)['behavior'][1:]\n",
    "\n",
    "# compare\n",
    "metric = load_metric('pearsonr')\n",
    "score = metric(reading_times1, reading_times2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural comparison\n",
    "\n",
    "Similarly, you can compare how similar neural activity is in two models.\n",
    "Here, we compare two artificial subject models stemming from the same base model by choosing different layers, but you can also compare different models altogether like above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "cross-validation:   0%|                                                    | 0/10 [00:00<?, ?it/s]\u001B[A\n",
      "cross-validation:  10%|████▍                                       | 1/10 [00:01<00:09,  1.11s/it]\u001B[A\n",
      "cross-validation:  20%|████████▊                                   | 2/10 [00:02<00:09,  1.18s/it]\u001B[A\n",
      "cross-validation:  30%|█████████████▏                              | 3/10 [00:03<00:08,  1.28s/it]\u001B[A\n",
      "cross-validation:  40%|█████████████████▌                          | 4/10 [00:04<00:07,  1.25s/it]\u001B[A\n",
      "cross-validation:  50%|██████████████████████                      | 5/10 [00:06<00:06,  1.24s/it]\u001B[A\n",
      "cross-validation:  60%|██████████████████████████▍                 | 6/10 [00:07<00:04,  1.24s/it]\u001B[A\n",
      "cross-validation:  70%|██████████████████████████████▊             | 7/10 [00:08<00:03,  1.24s/it]\u001B[A\n",
      "cross-validation:  80%|███████████████████████████████████▏        | 8/10 [00:09<00:02,  1.16s/it]\u001B[A\n",
      "cross-validation:  90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.16s/it]\u001B[A\n",
      "cross-validation: 100%|███████████████████████████████████████████| 10/10 [00:11<00:00,  1.19s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score ()>\n",
      "array(0.62209412)\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (split: 10, neuroid: 768)>\\narray([[ 0.95910202, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from brainscore_language import ArtificialSubject, load_metric\n",
    "from brainscore_language.model_helpers.huggingface import HuggingfaceSubject\n",
    "\n",
    "# load models\n",
    "model1 = HuggingfaceSubject(model_id='distilgpt2', region_layer_mapping={\n",
    "    ArtificialSubject.RecordingTarget.language_system: 'transformer.h.4.ln_1'})\n",
    "model2 = HuggingfaceSubject(model_id='distilgpt2', region_layer_mapping={\n",
    "    ArtificialSubject.RecordingTarget.language_system: 'transformer.h.5.ln_1'})\n",
    "\n",
    "# record neural activity\n",
    "model1.start_neural_recording(recording_target=ArtificialSubject.RecordingTarget.language_system,\n",
    "                                recording_type=ArtificialSubject.RecordingType.fMRI)\n",
    "model2.start_neural_recording(recording_target=ArtificialSubject.RecordingTarget.language_system,\n",
    "                                recording_type=ArtificialSubject.RecordingType.fMRI)\n",
    "text = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog',\n",
    "        'Waltz', 'bad', 'nymph', 'for', 'quick', 'jigs', 'vex',\n",
    "        \"Glib\", \"jocks\", \"quiz\", \"nymph\", \"to\", \"vex\", \"dwarf\",\n",
    "        \"Sphinx\", \"of\", \"black\", \"quartz,\", \"judge\", \"my\", \"vow\",\n",
    "        \"How\", \"vexingly\", \"quick\", \"daft\", \"zebras\", \"jump!\"]\n",
    "activity1 = model1.digest_text(text)['neural']\n",
    "activity2 = model2.digest_text(text)['neural']\n",
    "activity1['stimulus_id'] = activity2['stimulus_id'] = 'presentation', np.arange(len(text))\n",
    "\n",
    "# compare\n",
    "metric = load_metric('linear_pearsonr')\n",
    "score = metric(activity1, activity2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Human-to-human comparison\n",
    "\n",
    "As with the model-to-model comparisons, you can compare humans to one another.\n",
    "In this case, the data has already been recorded so we simply compare two sets of data to one another.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Behavioral comparison\n",
    "\n",
    "Using data from Futrell et al. 2018, we can compare how similar the reading times of half the subjects are to the reading times of the other half of subjects (this is also part of how the ceiling is estimated in the [Futrell2018 reading times benchmark](https://github.com/brain-score/language/blob/main/brainscore_language/benchmarks/futrell2018/__init__.py)):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.NeuroidAssembly 'data' (presentation: 10256, subject: 180)>\n",
      "array([[ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n",
      "       ...,\n",
      "       [512., 334., 283., ...,  nan,  nan,  nan],\n",
      "       [432., 390., 590., ...,  nan,  nan,  nan],\n",
      "       [576., 750., 862., ...,  nan,  nan,  nan]])\n",
      "Coordinates:\n",
      "  * presentation             (presentation) MultiIndex\n",
      "  - word                     (presentation) object 'If' 'you' ... \"Tourette's.\"\n",
      "  - word_core                (presentation) object 'If' 'you' ... 'Tourettes'\n",
      "  - story_id                 (presentation) int64 1 1 1 1 1 1 ... 10 10 10 10 10\n",
      "  - word_id                  (presentation) int64 1 2 3 4 5 ... 936 937 938 939\n",
      "  - word_within_sentence_id  (presentation) int64 1 2 3 4 5 6 ... 12 13 14 15 16\n",
      "  - sentence_id              (presentation) int64 1 1 1 1 1 ... 481 481 481 481\n",
      "  - stimulus_id              (presentation) int64 1 2 3 4 ... 10254 10255 10256\n",
      "  * subject                  (subject) MultiIndex\n",
      "  - subject_id               (subject) object 'A1INWCFGQI236V' ... 'A2VG5S4UL...\n",
      "  - correct                  (subject) int64 6 6 5 6 6 6 6 5 ... 6 5 5 6 6 6 6 5\n",
      "  - WorkTimeInSeconds        (subject) int64 1569 2748 2409 ... 1174 523 370\n",
      "Attributes:\n",
      "    identifier:  Futrell2018\n",
      "    bibtex:      @proceedings{futrell2018natural,\\n  title={The Natural Stori...\n"
     ]
    }
   ],
   "source": [
    "from brainscore_language import load_dataset\n",
    "\n",
    "data = load_dataset('Futrell2018')\n",
    "print(data)  # will show lots of nans because not every subject has a reading time for every word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score ()>\n",
      "array(0.61563052)\n",
      "Attributes:\n",
      "    rvalue:   0.6156305242624502\n",
      "    pvalue:   0.0\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import RandomState\n",
    "from brainscore_language import load_metric\n",
    "\n",
    "# split into halves\n",
    "random = RandomState(0)\n",
    "subjects = data['subject_id'].values\n",
    "half1_subjects = random.choice(subjects, size=len(subjects) // 2, replace=False)\n",
    "half2_subjects = set(subjects) - set(half1_subjects)\n",
    "half1 = data[{'subject': [subject_id in half1_subjects for subject_id in subjects]}]\n",
    "half2 = data[{'subject': [subject_id in half2_subjects for subject_id in subjects]}]\n",
    "\n",
    "# mean within each half\n",
    "half1 = half1.mean('subject')\n",
    "half2 = half2.mean('subject')\n",
    "\n",
    "# compare\n",
    "metric = load_metric('pearsonr')\n",
    "score = metric(half1, half2)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural comparison\n",
    "\n",
    "Using data from Pereira et al. 2018, we can test how well a pool of subjects can linearly predict a held-out subject (this is also part of how the ceiling is estimated in the [Pereira2018 linear predictivity benchmark](https://github.com/brain-score/language/blob/main/brainscore_language/benchmarks/pereira2018/__init__.py)):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.NeuroidAssembly 'data' (presentation: 384, neuroid: 12155)>\n",
      "array([[-0.61048431, -0.76491186, -0.79946189, ..., -1.02996304,\n",
      "        -0.42042251,  0.44600733],\n",
      "       [-0.57701107, -0.24646438, -0.28553028, ..., -0.29127415,\n",
      "        -0.10866586,  1.67496226],\n",
      "       [ 0.5322871 ,  0.69422809,  0.29570084, ...,  0.64426824,\n",
      "         0.0268965 ,  5.96437518],\n",
      "       ...,\n",
      "       [ 0.4911479 ,  0.97394189,  0.14704561, ...,  0.97622657,\n",
      "         1.07466326,  0.65786844],\n",
      "       [ 1.0331004 ,  1.5348565 ,  0.84328902, ..., -0.8361398 ,\n",
      "        -0.52408963,  0.73715778],\n",
      "       [ 0.53970481,  0.98636439,  0.53409886, ..., -0.03957355,\n",
      "        -0.06988947,  2.10925683]])\n",
      "Coordinates:\n",
      "  * presentation      (presentation) MultiIndex\n",
      "  - stimulus_num      (presentation) int64 0 1 2 3 4 5 ... 379 380 381 382 383\n",
      "  - sentence          (presentation) object 'An accordion is a portable music...\n",
      "  - stimulus          (presentation) object 'An accordion is a portable music...\n",
      "  - passage_index     (presentation) int64 1 1 1 1 2 2 2 ... 95 95 96 96 96 96\n",
      "  - passage_label     (presentation) object 'Accordion' 'Accordion' ... 'Woman'\n",
      "  - passage_category  (presentation) object 'music' 'music' ... 'human' 'human'\n",
      "  - stimulus_id       (presentation) object '384sentences.0' ... '384sentence...\n",
      "  - story             (presentation) object '384sentences.music' ... '384sent...\n",
      "  * neuroid           (neuroid) MultiIndex\n",
      "  - subject           (neuroid) object '018' '018' '018' ... '426' '426' '426'\n",
      "  - voxel_num         (neuroid) int64 28 29 31 32 38 ... 29796 29797 29798 30493\n",
      "  - filter_strategy   (neuroid) float64 nan nan nan nan nan ... nan nan nan nan\n",
      "  - atlas_selection   (neuroid) object 'from90to100prcnt' ... 'from90to100prcnt'\n",
      "  - roi               (neuroid) object 'LH_AntTemp' 'LH_AntTemp' ... 'LH_MFG'\n",
      "  - indices_in_3d     (neuroid) int64 72505 72506 72584 ... 395019 395020 402487\n",
      "  - col_to_coord_1    (neuroid) int64 62 63 62 63 63 63 63 ... 61 62 18 19 20 61\n",
      "  - col_to_coord_2    (neuroid) int64 63 63 64 64 65 66 63 ... 60 60 61 61 61 60\n",
      "  - col_to_coord_3    (neuroid) int64 10 10 10 10 10 10 11 ... 53 53 53 53 53 54\n",
      "  - neuroid_id        (neuroid) object '018.28' '018.29' ... '426.30493'\n",
      "Attributes:\n",
      "    identifier:  Pereira2018.language\n"
     ]
    }
   ],
   "source": [
    "from brainscore_language import load_dataset\n",
    "\n",
    "data = load_dataset('Pereira2018.language')\n",
    "data = data.sel(experiment='384sentences')\n",
    "data = data.dropna('neuroid')\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "cross-validation:   0%|                                                    | 0/10 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation:  10%|████▍                                       | 1/10 [00:05<00:50,  5.66s/it]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation:  20%|████████▊                                   | 2/10 [00:11<00:44,  5.53s/it]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation:  30%|█████████████▏                              | 3/10 [00:16<00:38,  5.47s/it]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation:  40%|█████████████████▌                          | 4/10 [00:21<00:32,  5.36s/it]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation:  50%|██████████████████████                      | 5/10 [00:27<00:27,  5.55s/it]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation:  60%|██████████████████████████▍                 | 6/10 [00:33<00:22,  5.54s/it]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation:  70%|██████████████████████████████▊             | 7/10 [00:38<00:16,  5.39s/it]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation:  80%|███████████████████████████████████▏        | 8/10 [00:43<00:10,  5.39s/it]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation:  90%|███████████████████████████████████████▌    | 9/10 [00:48<00:05,  5.26s/it]\u001B[A\u001B[A\n",
      "\n",
      "cross-validation: 100%|███████████████████████████████████████████| 10/10 [00:53<00:00,  5.39s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Score ()>\n",
      "array(0.35985278)\n",
      "Attributes:\n",
      "    raw:      <xarray.Score (split: 10, neuroid: 1357)>\\narray([[ 0.275236  ,...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from brainscore_language import load_metric\n",
    "\n",
    "heldout_subject = '426'\n",
    "\n",
    "pool = data[{'neuroid': [subject != heldout_subject for subject in data['subject'].values]}]\n",
    "heldout = data[{'neuroid': [subject == heldout_subject for subject in data['subject'].values]}]\n",
    "\n",
    "metric = load_metric('linear_pearsonr')\n",
    "score = metric(pool, heldout)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
