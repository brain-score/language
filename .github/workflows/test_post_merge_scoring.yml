name: Test Post-Merge Scoring

# Manual workflow to test Jenkins scoring trigger without going through full pipeline
on:
  workflow_dispatch:
    inputs:
      plugin_dir:
        description: 'Plugin directory path (e.g., brainscore_language/models/test_embedding_2)'
        required: true
        type: string
      plugin_type:
        description: 'Plugin type (models, benchmarks, etc.)'
        required: true
        type: choice
        options:
          - models
          - benchmarks
          - metrics
          - data
      test_email:
        description: 'Test email address'
        required: true
        type: string
      pr_number:
        description: 'PR number (optional - for extracting email from PR instead of using test_email)'
        required: false
        type: string

permissions:
  contents: read
  pull-requests: read

env:
  DOMAIN: language
  DOMAIN_ROOT: brainscore_language
  PYTHON_VERSION: '3.11'

jobs:
  test_scoring:
    name: Test Post-Merge Scoring
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools
          python -m pip install "." PyYAML

      - name: Extract user email
        id: extract_email
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          BSC_DATABASESECRET: ${{ secrets.BSC_DATABASESECRET }}
        run: |
          if [ -n "${{ inputs.pr_number }}" ]; then
            # Extract email from PR
            PR_USERNAME=$(gh pr view ${{ inputs.pr_number }} --json author -q '.author.login')
            PR_TITLE=$(gh pr view ${{ inputs.pr_number }} --json title -q '.title')
            EMAIL=$(python brainscore_language/submission/actions_helpers.py extract_email \
              --pr-username "$PR_USERNAME" \
              --pr-title "$PR_TITLE" \
              --is-automerge-web false || echo "")
            if [ -z "$EMAIL" ]; then
              echo "Warning: Could not extract email from PR, using test_email"
              EMAIL="${{ inputs.test_email }}"
            fi
          else
            # Use provided test email
            EMAIL="${{ inputs.test_email }}"
          fi
          echo "::add-mask::$EMAIL"
          echo "email=$EMAIL" >> $GITHUB_OUTPUT
          echo "Using email: $EMAIL"

      - name: Read metadata and layer mapping files
        id: read_metadata
        run: |
          PLUGIN_DIRS="${{ inputs.plugin_dir }}"
          
          # Write Python script to read metadata
          cat > /tmp/read_metadata.py << 'PYTHON_SCRIPT'
          import yaml
          import json
          import os
          import sys
          
          plugin_dirs_str = os.environ.get('PLUGIN_DIRS', '')
          metadata_dict = {}
          
          if plugin_dirs_str:
              plugin_dirs = [d.strip() for d in plugin_dirs_str.split(',') if d.strip()]
              
              for plugin_dir in plugin_dirs:
                  if not plugin_dir:
                      continue
                      
                  plugin_name = os.path.basename(plugin_dir.rstrip('/'))
                  metadata_file = None
                  
                  # Check for metadata.yml or metadata.yaml
                  if os.path.isfile(os.path.join(plugin_dir, 'metadata.yml')):
                      metadata_file = os.path.join(plugin_dir, 'metadata.yml')
                  elif os.path.isfile(os.path.join(plugin_dir, 'metadata.yaml')):
                      metadata_file = os.path.join(plugin_dir, 'metadata.yaml')
                  
                  if metadata_file:
                      try:
                          with open(metadata_file, 'r') as f:
                              metadata_content = yaml.safe_load(f)
                              if metadata_content:
                                  metadata_dict[plugin_name] = metadata_content
                      except Exception as e:
                          print(f'Error reading {metadata_file}: {e}', file=sys.stderr)
          
          # Build the final structure
          result = {
              'metadata': metadata_dict,
              'layer_mapping': None  # Language domain doesn't have layer mapping
          }
          
          print(json.dumps(result))
          PYTHON_SCRIPT
          
          # Execute the script
          export PLUGIN_DIRS
          METADATA_AND_LAYER_MAP=$(python /tmp/read_metadata.py 2>/dev/null || echo '{"metadata": {}, "layer_mapping": null}')
          
          # Base64 encode for safe storage
          METADATA_AND_LAYER_MAP_B64=$(echo "$METADATA_AND_LAYER_MAP" | base64 | tr -d '\n')
          echo "metadata_and_layer_map_b64=${METADATA_AND_LAYER_MAP_B64}" >> $GITHUB_OUTPUT
          echo "Read metadata for plugins: $PLUGIN_DIRS"
          echo "Metadata structure: $(echo "$METADATA_AND_LAYER_MAP" | jq -c .)"

      - name: Build plugin info
        id: build_info
        run: |
          # Build minimal plugin_info JSON for testing
          # This mimics what detect_changes would output
          PLUGIN_NAME=$(basename "${{ inputs.plugin_dir }}")
          
          # Decode metadata_and_layer_map
          METADATA_AND_LAYER_MAP_B64='${{ steps.read_metadata.outputs.metadata_and_layer_map_b64 }}'
          METADATA_AND_LAYER_MAP=$(echo "$METADATA_AND_LAYER_MAP_B64" | base64 -d)
          
          # Determine new_models/new_benchmarks based on plugin_type
          if [ "${{ inputs.plugin_type }}" == "models" ]; then
            NEW_MODELS="$PLUGIN_NAME"
            NEW_BENCHMARKS=""
          elif [ "${{ inputs.plugin_type }}" == "benchmarks" ]; then
            NEW_MODELS=""
            NEW_BENCHMARKS="$PLUGIN_NAME"
          else
            NEW_MODELS=""
            NEW_BENCHMARKS=""
          fi
          
          # Build changed_plugins structure
          if [ "${{ inputs.plugin_type }}" == "models" ]; then
            CHANGED_PLUGINS=$(jq -n -c --arg name "$PLUGIN_NAME" '{models: [$name]}')
          elif [ "${{ inputs.plugin_type }}" == "benchmarks" ]; then
            CHANGED_PLUGINS=$(jq -n -c --arg name "$PLUGIN_NAME" '{benchmarks: [$name]}')
          else
            CHANGED_PLUGINS=$(jq -n -c '{}')
          fi
          
          PLUGIN_INFO=$(jq -n -c \
            --arg domain "language" \
            --arg email "${{ steps.extract_email.outputs.email }}" \
            --arg plugin_dirs "${{ inputs.plugin_dir }}" \
            --arg plugin_type "${{ inputs.plugin_type }}" \
            --arg new_models "$NEW_MODELS" \
            --arg new_benchmarks "$NEW_BENCHMARKS" \
            --argjson changed_plugins "$CHANGED_PLUGINS" \
            --argjson metadata_and_layer_map "$METADATA_AND_LAYER_MAP" \
            '{
              modifies_plugins: true,
              changed_plugins: $changed_plugins,
              test_all_plugins: [],
              is_automergeable: true,
              run_score: "True",
              new_models: $new_models,
              new_benchmarks: $new_benchmarks,
              domain: $domain,
              email: $email,
              plugin_dirs: $plugin_dirs,
              plugin_type: $plugin_type,
              competition: "None",
              model_type: "artificialsubject",
              public: true,
              metadata_and_layer_map: $metadata_and_layer_map
            }')
          
          # Store PLUGIN_INFO in GITHUB_ENV for next step (contains email - will be masked by GitHub Actions)
          echo "PLUGIN_INFO=${PLUGIN_INFO}" >> $GITHUB_ENV
          echo "plugin_info=${PLUGIN_INFO}" >> $GITHUB_OUTPUT
          
          # Mask email and other sensitive fields in log output
          PLUGIN_INFO_MASKED=$(echo "$PLUGIN_INFO" | jq -c 'if .email then .email = "***MASKED***" else . end' 2>/dev/null || echo "{}")
          echo "Plugin info (sensitive data masked): ${PLUGIN_INFO_MASKED}"

      - name: Trigger Jenkins scoring
        env:
          JENKINS_USER: ${{ secrets.JENKINS_USER }}
          JENKINS_TOKEN: ${{ secrets.JENKINS_TOKEN }}
          JENKINS_TRIGGER: ${{ secrets.JENKINS_TRIGGER }}
          PLUGIN_INFO: ${{ steps.build_info.outputs.plugin_info }}
        run: |
          # PLUGIN_INFO contains email - GitHub Actions automatically masks secrets in logs
          # Suppress stdout/stderr to prevent any potential exposure of PLUGIN_INFO content
          python -c 'from brainscore_language.submission.endpoints import call_jenkins_language; import os; call_jenkins_language(os.environ["PLUGIN_INFO"])' >/dev/null 2>&1 || {
            echo "Jenkins scoring trigger failed (check Jenkins logs for details)"
            exit 1
          }
          echo "Jenkins scoring job triggered successfully"
